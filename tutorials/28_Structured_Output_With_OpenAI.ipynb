{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVBtOVlNJ51C"
   },
   "source": [
    "# Tutorial: Generating Structured Output with OpenAIChatGenerator\n",
    "\n",
    "- **Level**: Intermediate\n",
    "- **Time to complete**: 15 minutes\n",
    "- **Prerequisites**: You must have an API key from an active OpenAI account as this tutorial is using the gpt-4o-mini model by OpenAI.\n",
    "- **Components Used**: `ChatPromptBuilder`, `OpenAIChatGenerator`, `OutputValidator` (Custom component)\n",
    "- **Goal**: Learn how to generate structured outputs with `OpenAIChatGenerator` using Pydantic model or Json schema. Optionally, use a custom `OutputValidator` to confirm the output matches the provided Pydantic model.\n",
    "\n",
    "## Overview\n",
    "This tutorial shows how to produce structured outputs by either providing [Pydantic](https://github.com/pydantic/pydantic) model or JSON schema to `OpenAIChatGenerator`. Weâ€™ll use `OutputValidator` to verify the generated output against the schema.\n",
    "\n",
    "Note: Only latest model starting with `gpt-4o-mini` can be used for this feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmiAHh1oGsKI"
   },
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "Enable the debug mode of logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vor9IHuNRvEh"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"canals.pipeline.pipeline\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljbWiyJkKiPw"
   },
   "source": [
    "## Installing Dependencies\n",
    "Install Haystack and [colorama](https://pypi.org/project/colorama/) with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install \"haystack-ai>=2.18.0\"\n",
    "pip install colorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cmjfa8CiCeFl"
   },
   "source": [
    "## Structured output using Pydantic Model\n",
    "\n",
    "First, we'll see how to pass Pydantic model to `OpenAIChatGenerator`. For this purpose, we define two [Pydantic models](https://docs.pydantic.dev/1.10/usage/models/), `City` and `CitiesData`. These models specify the fields and types that represent the data structure we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xwKrDOOGdaAz"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class City(BaseModel):\n",
    "    name: str\n",
    "    country: str\n",
    "    population: int\n",
    "\n",
    "\n",
    "class CitiesData(BaseModel):\n",
    "    cities: List[City]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv-6-l_PCeFl"
   },
   "source": [
    "> You can change these models according to the format you wish to extract from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvNhg0bP7kfg"
   },
   "source": [
    "## Creating a Custom Component: OutputValidator\n",
    "\n",
    "`OutputValidator` is a custom component that validates if the JSON object the LLM generates complies with the provided [Pydantic model](https://docs.pydantic.dev/1.10/usage/models/). If it doesn't, OutputValidator returns an error message.\n",
    "\n",
    "For more details about custom components, see [Creating Custom Components](https://docs.haystack.deepset.ai/docs/custom-components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yr6D8RN2d7Vy"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pydantic\n",
    "from pydantic import ValidationError\n",
    "from typing import Optional, List\n",
    "from colorama import Fore\n",
    "from haystack import component\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "\n",
    "# Define the component input parameters\n",
    "@component\n",
    "class OutputValidator:\n",
    "    def __init__(self, pydantic_model: pydantic.BaseModel):\n",
    "        self.pydantic_model = pydantic_model\n",
    "        self.iteration_counter = 0\n",
    "\n",
    "    # Define the component output\n",
    "    @component.output_types(valid_replies=List[str], invalid_replies=Optional[List[str]], error_message=Optional[str])\n",
    "    def run(self, replies: List[ChatMessage]):\n",
    "\n",
    "        ## Try to parse the LLM's reply ##\n",
    "        # If the LLM's reply is a valid object, return `\"valid_replies\"`\n",
    "        try:\n",
    "            output_dict = json.loads(replies[0].text)\n",
    "            self.pydantic_model.model_validate(output_dict)\n",
    "            print(\n",
    "                Fore.GREEN\n",
    "                + f\"Valid JSON from LLM\"\n",
    "            )\n",
    "            return {\"valid_replies\": replies}\n",
    "\n",
    "        # If the LLM's reply is corrupted or not valid, return \"invalid_replies\" and the \"error_message\"\n",
    "        except (ValueError, ValidationError) as e:\n",
    "            print(\n",
    "                Fore.RED\n",
    "                + f\"Output from LLM:\\n {replies[0]} \\n\"\n",
    "                f\"Error from OutputValidator: {e}\"\n",
    "            )\n",
    "            return {\"invalid_replies\": replies, \"error_message\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ_TfSBkCeFm"
   },
   "source": [
    "Then, create an OutputValidator instance with `CitiesData` that you have created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bhPCLCBCCeFm"
   },
   "outputs": [],
   "source": [
    "output_validator = OutputValidator(pydantic_model=CitiesData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcIWKjW4k42r"
   },
   "source": [
    "## Create a Prompt for LLM\n",
    "\n",
    "Use `ChatPromptBuilder` in the pipeline to pass the userâ€™s message to `OpenAIChatGenerator`.\n",
    "For information about Jinja2 template and ChatPromptBuilder, see [ChatPromptBuilder](https://docs.haystack.deepset.ai/docs/chatpromptbuilder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ohPpNALjdVKt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.components.builders.chat_prompt_builder:ChatPromptBuilder has 1 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.builders import ChatPromptBuilder\n",
    "\n",
    "prompt_template = [ChatMessage.from_user(\"User Input: {{passage}}\")]\n",
    "prompt_builder = ChatPromptBuilder(template=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM9-Zq2FL7Nn"
   },
   "source": [
    "## Initalizing the ChatGenerator to produce structured outputs\n",
    "\n",
    "[OpenAIChatGenerator](https://docs.haystack.deepset.ai/docs/openaichatgenerator) generates\n",
    "text using OpenAI's `gpt-4o-mini` model by default. We pass our Pydantic model to `response_format` paramter in generation_kwargs .\n",
    "\n",
    "We also need to set the `OPENAI_API_KEY` variable.\n",
    "\n",
    "Note: You can also set the `response_format` param in the `run` method of chat generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z4cQteIgunUR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "chat_generator = OpenAIChatGenerator(generation_kwargs={\"response_format\": CitiesData})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbotIOgXHkC5"
   },
   "source": [
    "## Building the Pipeline\n",
    "\n",
    "Add all components to your pipeline and connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eFglN9YEv-1W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x10ca15730>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - llm: OpenAIChatGenerator\n",
       "  - output_validator: OutputValidator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.messages (list[ChatMessage])\n",
       "  - llm.replies -> output_validator.replies (list[ChatMessage])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "pipeline = Pipeline(max_runs_per_component=5)\n",
    "\n",
    "# Add components to your pipeline\n",
    "pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
    "pipeline.add_component(instance=chat_generator, name=\"llm\")\n",
    "pipeline.add_component(instance=output_validator, name=\"output_validator\")\n",
    "\n",
    "# Now, connect the components to each other\n",
    "pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
    "pipeline.connect(\"llm.replies\", \"output_validator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UKW5wtIIT7w"
   },
   "source": [
    "### Visualize the Pipeline\n",
    "\n",
    "Draw the pipeline with the [`draw()`](https://docs.haystack.deepset.ai/docs/drawing-pipeline-graphs) method to confirm the connections are correct. You can find the diagram in the Files section of this Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RZJg6YHId300"
   },
   "outputs": [],
   "source": [
    "# pipeline.draw(\"auto-correct-pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV_kexTjImpo"
   },
   "source": [
    "## Testing the Pipeline\n",
    "\n",
    "Run the pipeline with an example passage that you want to convert into a JSON format and the `json_schema` you have created for `CitiesData`. For the given example passage, the generated JSON object should be like:\n",
    "```json\n",
    "{\n",
    "  \"cities\": [\n",
    "    {\n",
    "      \"name\": \"Berlin\",\n",
    "      \"country\": \"Germany\",\n",
    "      \"population\": 3850809\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Paris\",\n",
    "      \"country\": \"France\",\n",
    "      \"population\": 2161000\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Lisbon\",\n",
    "      \"country\": \"Portugal\",\n",
    "      \"population\": 504718\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "The output of the LLM should be compliant with the `json_schema`. If the LLM doesn't generate the correct JSON object, it will loop back and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIoMedb6eKia",
    "outputId": "4a9ef924-cf26-4908-d83f-b0bc0dc03b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mValid JSON from LLM\n"
     ]
    }
   ],
   "source": [
    "passage = \"Berlin is the capital of Germany. It has a population of 3,850,809. Paris, France's capital, has 2.161 million residents. Lisbon is the capital and the largest city of Portugal with the population of 504,718.\"\n",
    "result = pipeline.run({\"prompt_builder\": {\"passage\": passage}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWPawSjgSJAM"
   },
   "source": [
    "### Print the Correct JSON\n",
    "If you didn't get any error, you can now print the corrected JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVO47gXQQnDC",
    "outputId": "460a10d4-a69a-49cd-bbb2-fc4980907299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cities': [{'name': 'Berlin', 'country': 'Germany', 'population': 3850809}, {'name': 'Paris', 'country': 'France', 'population': 2161000}, {'name': 'Lisbon', 'country': 'Portugal', 'population': 504718}]}\n"
     ]
    }
   ],
   "source": [
    "valid_reply = result[\"output_validator\"][\"valid_replies\"][0].text\n",
    "valid_json = json.loads(valid_reply)\n",
    "print(valid_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured output using Json schema\n",
    "\n",
    "Now, weâ€™ll create a json schema of the `CitiesData` model and pass it to `OpenAIChatGenerator`. OpenAI expects schemas in a specific format, so the schema generated with `model_json_schema()` cannot be used directly.\n",
    "\n",
    "For details on how to create schemas for OpenAI, see the [OpenAI Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs#supported-schemas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data_schema={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"CitiesData\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"cities\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": { \"type\": \"string\" },\n",
    "                                \"country\": { \"type\": \"string\" },\n",
    "                                \"population\": { \"type\": \"integer\" }\n",
    "                            },\n",
    "                            \"required\": [\"name\", \"country\", \"population\"],\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"cities\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass this json schema to the `response_format` parameter in chat generator. We run the generator indivdually to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cities\":[{\"name\":\"Berlin\",\"country\":\"Germany\",\"population\":3850809},{\"name\":\"Paris\",\"country\":\"France\",\"population\":2161000},{\"name\":\"Lisbon\",\"country\":\"Portugal\",\"population\":504718}]}\n"
     ]
    }
   ],
   "source": [
    "chat_generator = OpenAIChatGenerator(generation_kwargs={\"response_format\": cities_data_schema})\n",
    "\n",
    "messages = [ChatMessage.from_user(\"Berlin is the capital of Germany. It has a population of 3,850,809. Paris, France's capital, has 2.161 million residents. Lisbon is the capital and the largest city of Portugal with the population of 504,718.\")]\n",
    "\n",
    "result = chat_generator.run(messages=messages)\n",
    "\n",
    "print(result[\"replies\"][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egz_4h2vI_QL"
   },
   "source": [
    "## What's next\n",
    "\n",
    "ðŸŽ‰ Congratulations! You've learnt how to easily produce structured ouputs with `OpenAIChatGenerator` using Pydantic models and Json schema.\n",
    "\n",
    "To stay up to date on the latest Haystack developments, you can [subscribe to our newsletter](https://landing.deepset.ai/haystack-community-updates) and [join Haystack discord community](https://discord.gg/haystack).\n",
    "\n",
    "Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
