{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using Haystack with REST API\n",
    "\n",
    "- **Level**: Advanced\n",
    "- **Time to complete**: 30 minutes\n",
    "- **Prerequisites**: Basic understanding of Docker, basic knowledge of Haystack pipelines \n",
    "- **Nodes Used**: `ElasticsearchDocumentStore`, `EmbeddingRetriever`\n",
    "- **Goal**: Learn how you can interact with Haystack through REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Haystack enables you to apply the latest NLP technology to your own data and create production-ready applications. Building an end-to-end NLP application requires the combination of multiple concepts:\n",
    "* **DocumentStore** is the component in Haystack responsible for loading and storing text data in form of [Documents](https://docs.haystack.deepset.ai/docs/documents_answers_labels#document). In this tutorial, the DocumentStore will use Elasticsearch behind the scene.\n",
    "* **Haystack** pipelines convert files into Documents, index them to the DocumentStore, and run NLP tasks such as question answering and document search.\n",
    "* **REST API**, as a concept, allows applications to interact with each other by handling their queries and returning responses. There is `rest_api` application within Haystack that exposes Haystack's functionalities through a RESTful API.\n",
    "* **Docker** simplifies the environment setup needed to have Elasticsearch and Haystack API running.\n",
    "\n",
    "This tutorial introduces you to all the concepts needed to build an end-to-end document search application. After completing this tutorial, you will have learned how to create a pipeline YAML file, index files, and how to query your pipeline using REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Environment\n",
    "\n",
    "1. Install [Docker Compose](https://docs.docker.com/compose/), and launch Docker.\n",
    "\n",
    "If you installed Docker Desktop, you just need to start the application. Run `docker info` to see if Docker is up and running.\n",
    "\n",
    "```bash\n",
    "docker info\n",
    "```\n",
    "\n",
    "2. Download the `docker-compose.yml` file.\n",
    "\n",
    "Haystack provides a `docker-compose.yml` file that defines services for Haystack API and Elasticsearch. Create a new folder called `doc-search` and save the latest [`docker-compose.yml`](https://github.com/deepset-ai/haystack/blob/main/docker-compose.yml) file from GitHub into the folder. You can run the command below to save the `docker-compose.yml` file into the directory directly.\n",
    "\n",
    "```bash\n",
    "curl --output docker-compose.yml https://raw.githubusercontent.com/deepset-ai/haystack/main/docker-compose.yml\n",
    "```\n",
    "\n",
    "Here's how the `/doc-search` folder should look like:\n",
    "```\n",
    "/doc-search\n",
    "└── docker-compose.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Pipeline YAML File\n",
    "\n",
    "YAML files are widely used for configurations and Haystack makes no exception: you can define components and pipelines using YAML code that Haystack will eventually translate into Python objects. In a pipeline YAML file, the `components` section lists all pipeline nodes, while the `pipelines` section defines how these nodes are connected to each other. Let's start with defining two different pipelines, one to index your documents and another one to query them.\n",
    "\n",
    "1. Create a document search pipeline.\n",
    "\n",
    "Time to design a document search pipeline from scratch. This will be your query pipeline. Create a new file named `document-search.haystack-pipeline.yml` in newly created `doc-search` folder. The compose file and the new pipeline YAML file should be on the same level in the directory.\n",
    "\n",
    "```\n",
    "/doc-search\n",
    "├── docker-compose.yml\n",
    "└── document-search.haystack-pipeline.yml\n",
    "```\n",
    "\n",
    "Then, update the source of `volume` in the compose file. As the source value, you need to provide a path to `document-search.haystack-pipeline.yml` relative to `docker-compose.yml`. As they are in the same directory, the source value will be `./`. \n",
    "\n",
    "```yaml\n",
    "haystack-api:\n",
    "  ...\n",
    "  volumes:\n",
    "    - ./:/opt/pipelines\n",
    "```\n",
    "\n",
    "After updating the volume, update the `PIPELINE_YAML_PATH` variable in the `docker-compose.yml` with the new file name. The `PIPELINE_YAML_PATH` variable will tell `rest_api` which YAML file to run. \n",
    "\n",
    "```yaml\n",
    "environment:\n",
    "  ...\n",
    "  - PIPELINE_YAML_PATH=/opt/pipelines/document-search.haystack-pipeline.yml\n",
    "  ...\n",
    "```\n",
    "\n",
    "A document search pipeline requires a DocumentStore and a Retriever. Use `ElasticsearchDocumentStore` and `EmbeddingRetriever` for these nodes respectively and define them under `components`. For each component, set `type` to a node class in Haystack, set `name` to how you want to call this node in the pipeline YAML, and use `params` to set the node parameters. As DocumentStore parameters, provide `embedding_dim` required for the `embedding_model` and as Retriever parameters, provide `document_store`, `embedding_model`, and a `top_k` value. Let's start by defining the pipeline nodes:\n",
    "\n",
    "```yaml\n",
    "components:\n",
    "  - name: DocumentStore\n",
    "    type: ElasticsearchDocumentStore\n",
    "    params:\n",
    "      embedding_dim: 384\n",
    "  - name: Retriever\n",
    "    type: EmbeddingRetriever\n",
    "    params:\n",
    "      document_store: DocumentStore\n",
    "      top_k: 10\n",
    "      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n",
    "```\n",
    "\n",
    "After you define the nodes, create a query pipeline in the `pipelines` section. Here, `name` refers to the name of the pipeline, and `nodes` defines the order of the nodes in the pipeline: \n",
    "\n",
    "```yaml\n",
    "pipelines:\n",
    "  - name: query \n",
    "    nodes:\n",
    "      - name: Retriever\n",
    "        inputs: [Query]\n",
    "```\n",
    "\n",
    "2. Create an indexing pipeline.\n",
    "\n",
    "You can define an indexing pipeline in the same pipeline YAML file and index your documents to Elasticsearch through `rest_api`. For that, create `FileTypeClassifier`, `TextConverter`, and `PreProcessor` nodes. For `PreProcessor`, use `params` to define how you want to split your documents: \n",
    "\n",
    "```yaml\n",
    "components:\n",
    "  ...\n",
    "  - name: FileTypeClassifier\n",
    "    type: FileTypeClassifier\n",
    "  - name: TextFileConverter\n",
    "    type: TextConverter\n",
    "  - name: Preprocessor\n",
    "    type: PreProcessor\n",
    "    params:\n",
    "      split_by: word\n",
    "      split_length: 250\n",
    "      split_overlap: 30 \n",
    "      split_respect_sentence_boundary: True \n",
    "```\n",
    "\n",
    "Then, in the `pipelines` section of the YAML file, create a new pipeline called `indexing`. In this pipeline, indicate how the nodes you just defined are connected to each other, Retriever, and DocumentStore. This indexing pipeline supports `.TXT` files and pre-processes them before loading to the Elasticsearch.\n",
    "\n",
    "```yaml\n",
    "pipelines:\n",
    "  ...\n",
    "  - name: indexing\n",
    "    nodes:\n",
    "      - name: FileTypeClassifier\n",
    "        inputs: [File]\n",
    "      - name: TextFileConverter\n",
    "        inputs: [FileTypeClassifier.output_1]\n",
    "      - name: Preprocessor\n",
    "        inputs: [TextFileConverter]\n",
    "      - name: Retriever\n",
    "        inputs: [Preprocessor]\n",
    "      - name: DocumentStore\n",
    "        inputs: [Retriever]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing query and indexing pipelines, add `version: ignore` to the top of the file. Now, the pipeline YAML is ready.\n",
    "\n",
    "```yaml\n",
    "version: ignore\n",
    "\n",
    "components:\n",
    "  - name: DocumentStore\n",
    "    type: ElasticsearchDocumentStore\n",
    "    params:\n",
    "      embedding_dim: 384\n",
    "  - name: Retriever\n",
    "    type: EmbeddingRetriever\n",
    "    params:\n",
    "      document_store: DocumentStore\n",
    "      top_k: 10 \n",
    "      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n",
    "  - name: FileTypeClassifier\n",
    "    type: FileTypeClassifier\n",
    "  - name: TextFileConverter\n",
    "    type: TextConverter\n",
    "  - name: Preprocessor\n",
    "    type: PreProcessor\n",
    "    params:\n",
    "      split_by: word\n",
    "      split_length: 250\n",
    "      split_overlap: 30 \n",
    "      split_respect_sentence_boundary: True\n",
    "\n",
    "pipelines:\n",
    "  - name: query \n",
    "    nodes:\n",
    "      - name: Retriever\n",
    "        inputs: [Query]\n",
    "  - name: indexing\n",
    "    nodes:\n",
    "      - name: FileTypeClassifier\n",
    "        inputs: [File]\n",
    "      - name: TextFileConverter\n",
    "        inputs: [FileTypeClassifier.output_1]\n",
    "      - name: Preprocessor\n",
    "        inputs: [TextFileConverter]\n",
    "      - name: Retriever\n",
    "        inputs: [Preprocessor]\n",
    "      - name: DocumentStore\n",
    "        inputs: [Retriever]\n",
    "```\n",
    "\n",
    "Feel free to play with the pipeline setup later on. Add or remove some nodes, change the parameters, or add new ones. For more options for nodes and parameters, check out [Haystack API Reference](https://docs.haystack.deepset.ai/reference/answer-generator-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Haystack API and Elasticsearch\n",
    "\n",
    "Pipelines are ready. Now, run `docker-compose up` to start `elasticsearch` and `haystack-api` containers. This command will install all necessary packages, set up the environment, and launch both Elasticsearch and Haystack API. Mind that launching might take 2-3 minutes. \n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "Before continuing, test if everything is OK with the Haystack API by sending a cURL request to the `/initialized` endpoint. If everything works fine, you will get `true` as a response.\n",
    "\n",
    "```bash\n",
    "curl --request GET http://127.0.0.1:8000/initialized\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Files to Elasticsearch\n",
    "\n",
    "Right now, the Elasticsearch instance is empty. Haystack API provides a `/file-upload` endpoint to upload files to Elasticsearch using the indexing pipeline defined in the pipeline YAML. After indexing files to Elasticsearch, you can perform document search.\n",
    "\n",
    "1. Download example files.\n",
    "\n",
    "Download the [example files](https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/article_txt_countries_and_capitals.zip) into the `doc-search` folder. In the downloaded zip file, there are text files about countries and capitals crawled from [Wikipedia](https://en.wikipedia.org/wiki/Category:Lists_of_countries_by_continent).\n",
    "\n",
    "```\n",
    "/doc-search\n",
    "├── docker-compose.yml\n",
    "├── document-search.haystack-pipeline.yml\n",
    "└── /article_txt_countries_and_capitals\n",
    "    ├── 0_Minsk.txt\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "2. Index files to Elasticsearch.\n",
    "\n",
    "You can send cURL requests to the `/file-upload` endpoint to upload files to the Elasticsearch instance. If the file is successfully uploaded, you will get `null` as a response.\n",
    "\n",
    "```bash\n",
    "curl --request POST \\\n",
    "     --url http://127.0.0.1:8000/file-upload \\\n",
    "     --header 'accept: application/json' \\\n",
    "     --header 'content-type: multipart/form-data' \\\n",
    "     --form files=@article_txt_countries_and_capitals/0_Minsk.txt \\\n",
    "     --form meta=null\n",
    "```\n",
    "\n",
    "This method is not convenient for uploading multiple files to the Elasticsearch instance as replacing the file names in the request by hand is difficult. Instead, you can run a command that takes all `.TXT` files in the `article_txt_countries_and_capitals` folder and sends a POST request to index each file.   \n",
    "\n",
    "```bash\n",
    "find ./article_txt_countries_and_capitals -name '*.txt' -exec \\\n",
    "     curl --request POST \\\n",
    "          --url http://127.0.0.1:8000/file-upload \\\n",
    "          --header 'accept: application/json' \\\n",
    "          --header 'content-type: multipart/form-data' \\\n",
    "          --form files=\"@{}\" \\\n",
    "          --form meta=null \\;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voilà! Make a query!\n",
    "\n",
    "The application is ready! Send another POST request to retrieve documents about _\"climate in Scandinavia\"_. \n",
    "\n",
    "```bash\n",
    "curl --request POST \\\n",
    "     --url http://127.0.0.1:8000/query \\\n",
    "     --header 'accept: application/json' \\\n",
    "     --header 'content-type: application/json' \\\n",
    "     --data '{\n",
    "     \"query\": \"climate in Scandinavia\"\n",
    "     }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a response, you will get a `QueryResponse` object consisting of `query`, `answers`, and `documents`. Documents related to your query will be under the `documents` attribute of the object.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"query\": \"climate in Scandinavia\",\n",
    "  \"answers\": [],\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": \"52937ad257317032b9aed9750b5fcbb7\",\n",
    "      \"content\": \"Even though temperature patterns differ between north and south, the summer climate is surprisingly similar all through the entire country in spite of the large latitudinal differences. This is due to the south's being surrounded by a greater mass of water, with the wider Baltic Sea and the Atlantic air passing over lowland areas from the south-west. ...\",\n",
    "      \"content_type\": \"text\",\n",
    "      \"meta\": {\n",
    "        \"_split_id\": 7,\n",
    "        \"name\": \"43_Sweden.txt\"\n",
    "      },\n",
    "      \"score\": 0.569930352925387\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) tutorial was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
