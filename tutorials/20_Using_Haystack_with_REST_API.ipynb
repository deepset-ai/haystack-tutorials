{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using Haystack with REST API\n",
    "\n",
    "- **Level**: Advanced\n",
    "- **Time to complete**: 30 minutes\n",
    "- **Prerequisites**: Basic understanding of Docker, basic knowledge of Haystack pipelines \n",
    "- **Nodes Used**: `ElasticsearchDocumentStore`, `EmbeddingRetriever`\n",
    "- **Goal**: Learn how you can interact with Haystack through REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Haystack enables you to apply the latest NLP technology to your own data and create production-ready applications. Building an end-to-end NLP application requires the combination of multiple concepts:\n",
    "* **DocumentStore** is the component in Haystack responsible for loading and storing text data in form of [Documents](https://docs.haystack.deepset.ai/docs/documents_answers_labels#document). In this tutorial, the DocumentStore will use Elasticsearch behind the scene.\n",
    "* **Haystack** pipelines convert files into Documents, index them to DocumentStore, and run NLP tasks such as question answering and document search.\n",
    "* **REST API**, as a concept, allows applications to interact with each other by handling their queries and returning responses. There is `rest_api` application within Haystack that exposes Haystack's functionalities through a RESTful API.\n",
    "* **Docker** simplifies the environment setup needed to have Elasticsearch and Haystack API running.\n",
    "\n",
    "This tutorial introduces you to all the concepts needed to build an end-to-end document search application. After completing this tutorial, you will have learned how to create a pipeline YAML file, index files, and how to query your pipeline using REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install [Docker Compose](https://docs.docker.com/compose/), and launch Docker.\n",
    "\n",
    "If you installed Docker Desktop, you just need to start the application. Run `docker info` to see if Docker is up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "docker info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Clone Haystack repository.\n",
    "\n",
    "Haystack provides a `docker-compose.yml` file that defines services for Haystack API and Elasticsearch. Clone the Haystack repository to be able to run the `docker-compose.yml` file locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/deepset-ai/haystack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Update `docker-compose.yml` file.\n",
    "\n",
    "Find and open the `docker-compose.yml` file. In the file, there is a predefined volume under `haystack-api` service. You need to replace `<PATH_TO_HAYSTACK>` with the path to the folder where you cloned the Haystack repository. Here's an example volume:\n",
    "\n",
    "```yaml\n",
    "volumes:\n",
    "  - /Users/admin/Desktop/examples/haystack/rest_api/rest_api/pipeline:/home/user/rest_api/pipeline\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline YAML File\n",
    "\n",
    "YAML files are widely used for configurations. You can define Haystack pipelines as YAML files and then `load_from_yaml()` method loads the pipeline defined in YAML into a Python object. In a YAML file, the `components` section defines all pipeline nodes and the `pipelines` section defines how these nodes are connected to each other to form a pipeline. Let's start with defining query and indexing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a document search pipeline.\n",
    "\n",
    "Time to design a document search pipeline from scratch. This will be your query pipeline. Create a new file named `document-search.haystack-pipeline.yml` in the `/pipeline` folder under `/rest_api` in the Haystack code base. Then, create a `PIPELINE_YAML_PATH` variable in the `docker-compose.yml` with the new file name. `PIPELINE_YAML_PATH` variable will tell `rest_api` which YAML file to run. \n",
    "\n",
    "```yaml\n",
    "environment:\n",
    "  - DOCUMENTSTORE_PARAMS_HOST=elasticsearch\n",
    "  - PIPELINE_YAML_PATH=/home/user/rest_api/pipeline/document-search.haystack-pipeline.yml\n",
    "  ...\n",
    "```\n",
    "\n",
    "A document search pipeline requires a DocumentStore and a Retriever. Use `ElasticsearchDocumentStore` and `EmbeddingRetriever` for these nodes respectively and define them under `components`. For each component, set `type` to a node class in Haystack, set `name` to how you want to call this node in the pipeline YAML, and use `params` to set the node parameters. As DocumentStore parameters, provide `embedding_dim` required for the `embedding_model` and as Retriever parameters, provide `document_store`, `embedding_model`, and a `top_k` value. Let's start by defining the pipeline nodes:\n",
    "\n",
    "```yaml\n",
    "components:\n",
    "  - name: DocumentStore\n",
    "    type: ElasticsearchDocumentStore\n",
    "    params:\n",
    "      embedding_dim: 384\n",
    "  - name: Retriever\n",
    "    type: EmbeddingRetriever\n",
    "    params:\n",
    "      document_store: DocumentStore\n",
    "      top_k: 5 \n",
    "      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n",
    "```\n",
    "\n",
    "After you define the nodes, create a query pipeline in the `pipelines` section. Here, `name` refers to the name of the pipeline, and `nodes` defines the order of the nodes in the pipeline: \n",
    "\n",
    "```yaml\n",
    "pipelines:\n",
    "  - name: query \n",
    "    nodes:\n",
    "      - name: Retriever\n",
    "        inputs: [Query]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create an indexing pipeline.\n",
    "\n",
    "You can define an indexing pipeline in the same pipeline YAML file and index your documents to Elasticsearch through `rest_api`. For that, create `FileTypeClassifier`, `TextConverter`, `PDFToTextConverter`, and `PreProcessor` nodes. For `PreProcessor`, use `params` to define how you want to split your documents: \n",
    "\n",
    "```yaml\n",
    "components:\n",
    "  ...\n",
    "  - name: FileTypeClassifier\n",
    "    type: FileTypeClassifier\n",
    "  - name: TextFileConverter\n",
    "    type: TextConverter\n",
    "  - name: PDFFileConverter\n",
    "    type: PDFToTextConverter\n",
    "  - name: Preprocessor\n",
    "    type: PreProcessor\n",
    "    params:\n",
    "      split_by: word\n",
    "      split_length: 1000\n",
    "      split_overlap: 50 \n",
    "      split_respect_sentence_boundary: True \n",
    "```\n",
    "\n",
    "Then, in the `pipelines` section of the YAML file, create a new pipeline called `indexing`. In this pipeline, indicate how the nodes you just defined are connected to each other, Retriever, and DocumentStore. This indexing pipeline supports `.TXT` and `.PDF` files and pre-processes them before loading to the Elasticsearch.\n",
    "\n",
    "```yaml\n",
    "pipelines:\n",
    "  ...\n",
    "  - name: indexing\n",
    "    nodes:\n",
    "      - name: FileTypeClassifier\n",
    "        inputs: [File]\n",
    "      - name: TextFileConverter\n",
    "        inputs: [FileTypeClassifier.output_1]\n",
    "      - name: PDFFileConverter\n",
    "        inputs: [FileTypeClassifier.output_2]\n",
    "      - name: Preprocessor\n",
    "        inputs: [PDFFileConverter, TextFileConverter]\n",
    "      - name: Retriever\n",
    "        inputs: [Preprocessor]\n",
    "      - name: DocumentStore\n",
    "        inputs: [Retriever]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing query and indexing pipelines, add `version: ignore` to the top of the file. Now, the pipeline YAML is ready.\n",
    "\n",
    "```yaml\n",
    "version: ignore\n",
    "\n",
    "components:\n",
    "  - name: DocumentStore\n",
    "    type: ElasticsearchDocumentStore\n",
    "    params:\n",
    "      embedding_dim: 384\n",
    "  - name: Retriever\n",
    "    type: EmbeddingRetriever\n",
    "    params:\n",
    "      document_store: DocumentStore\n",
    "      top_k: 5 \n",
    "      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n",
    "  - name: FileTypeClassifier\n",
    "    type: FileTypeClassifier\n",
    "  - name: TextFileConverter\n",
    "    type: TextConverter\n",
    "  - name: PDFFileConverter\n",
    "    type: PDFToTextConverter\n",
    "  - name: Preprocessor\n",
    "    type: PreProcessor\n",
    "    params:\n",
    "      split_by: word\n",
    "      split_length: 1000\n",
    "      split_overlap: 50 \n",
    "      split_respect_sentence_boundary: True\n",
    "\n",
    "pipelines:\n",
    "  - name: query \n",
    "    nodes:\n",
    "      - name: Retriever\n",
    "        inputs: [Query]\n",
    "  - name: indexing\n",
    "    nodes:\n",
    "      - name: FileTypeClassifier\n",
    "        inputs: [File]\n",
    "      - name: TextFileConverter\n",
    "        inputs: [FileTypeClassifier.output_1]\n",
    "      - name: PDFFileConverter\n",
    "        inputs: [FileTypeClassifier.output_2]\n",
    "      - name: Preprocessor\n",
    "        inputs: [PDFFileConverter, TextFileConverter]\n",
    "      - name: Retriever\n",
    "        inputs: [Preprocessor]\n",
    "      - name: DocumentStore\n",
    "        inputs: [Retriever]\n",
    "```\n",
    "\n",
    "Feel free to play with the pipeline setup later on. Add or remove some nodes, change the parameters, or add new ones. For more options for nodes and parameters, check out [Haystack API Reference](https://docs.haystack.deepset.ai/reference/answer-generator-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Haystack API and Elasticsearch\n",
    "\n",
    "Pipelines are ready. Now, go to the directory where `docker-compose.yml` is and run `docker-compose up`. This command will install all necessary packages, set up the environment, and launch both Elasticsearch and Haystack API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "docker-compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, test if everything is OK with the Haystack API by sending a cURL request to the `/initialized` endpoint. You can use the command line of your computer or tools like [Postman](https://www.postman.com/) to send cURL requests. If everything works fine, you will get `true` as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl --request GET http://127.0.0.1:8000/initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Files to Elasticsearch\n",
    "\n",
    "Right now, the Elasticsearch instance is empty. Haystack API provides a `/file-upload` endpoint to upload files to Elasticsearch using the indexing pipeline defined in the pipeline YAML. After indexing files to Elasticsearch, you will be able to perform document search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download example files.\n",
    "\n",
    "Download the [example files](https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/article_txt_countries_and_capitals.zip) you will be indexing to Elasticsearch. In the folder, there are text files about countries and capitals crawled from [Wikipedia](https://en.wikipedia.org/wiki/Category:Lists_of_countries_by_continent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Index files to Elasticsearch.\n",
    "\n",
    "You can send cURL requests to the `/file-upload` endpoint to upload files to the Elasticsearch instance. Replace `<PATH_TO_FOLDER>` with the path to the example files on your computer and send the POST request. If the file is successfully uploaded, you will get `null` as a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl --request POST \\\n",
    "     --url http://127.0.0.1:8000/file-upload \\\n",
    "     --header 'accept: application/json' \\\n",
    "     --header 'content-type: multipart/form-data' \\\n",
    "     --form files=@<PATH_TO_FOLDER>0_Minsk.txt \\\n",
    "     --form meta=null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is not convenient for uploading multiple files to the Elasticsearch instance as replacing the file names in the request by hand is difficult. Instead, create a python file in the folder where you keep the example files, put the code below into the python file, and run the python script. This python code takes the name of every file in the folder and sends the POST request to the URL `http://127.0.0.1:8000/file-upload`. With the `print(response.text, \":\", file_name)` line, you will be able to see the names of all indexed files. Make sure you see the \"Completed\" text before continuing.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import requests\n",
    " \n",
    "file_list = os.listdir()\n",
    "url = \"http://127.0.0.1:8000/file-upload\"\n",
    "payload = {\"meta\": \"null\"}\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "for file_name in file_list:\n",
    "    files = {\"files\": (file_name, open(file_name, \"rb\"), \"text/plain\")}\n",
    "    response = requests.post(url, data=payload, files=files, headers=headers)\n",
    "    print(response.text, \":\", file_name)\n",
    "\n",
    "print(\"Completed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voilà! Make a query!\n",
    "\n",
    "The application is ready! Send a cURL request to retrieve documents about _\"climate in Scandinavia\"_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl --request POST \\\n",
    "     --url http://127.0.0.1:8000/query \\\n",
    "     --header 'accept: application/json' \\\n",
    "     --header 'content-type: application/json' \\\n",
    "     --data '{\n",
    "     \"query\": \"climate in Scandinavia\"\n",
    "     }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a response, you will get a `QueryResponse` object consisting of `query`, `answers`, and `documents`. Documents related to your query will be under the `documents` attribute of the object.\n",
    "\n",
    "```bash\n",
    "{\n",
    "  \"query\": \"climate in Scandinavia\",\n",
    "  \"answers\": [],\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": \"52937ad257317032b9aed9750b5fcbb7\",\n",
    "      \"content\": \"Even though temperature patterns differ between north and south, the summer climate is surprisingly similar all through the entire country in spite of the large latitudinal differences. This is due to the south's being surrounded by a greater mass of water, with the wider Baltic Sea and the Atlantic air passing over lowland areas from the south-west. ...\",\n",
    "      \"content_type\": \"text\",\n",
    "      \"meta\": {\n",
    "        \"_split_id\": 7,\n",
    "        \"name\": \"43_Sweden.txt\"\n",
    "      },\n",
    "      \"score\": 0.569930352925387\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) tutorial was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
