{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTas9ZQ7lXP7"
   },
   "source": [
    "# Tutorial: Create an Hybrid Retrieval Pipeline\n",
    "\n",
    "- **Level**: Intermediate\n",
    "- **Time to complete**: 15\n",
    "- **Nodes Used**: `EmbeddingRetriever`, `BM25Retriever`, `JoinDocuments`, `SentenceTransformersRanker` and `InMemoryDocumentStore`, `Document`\n",
    "- **Goal**: After completing this tutorial, you will have learned about create your first hybrid retrieval and when it's useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hw_zoKolXQL"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**Hybrid Retrieval** merges dense and sparse vectors together to deliver the best of both search methods. Generally speaking, dense vectors excel at understanding the context of the query, whereas sparse vectors excel at keyword matches.\n",
    "There are many cases when a simple sparse retrieval like BM25 performs better than a dense retrieval (for example in a specific domain like healthcare) because a dense encoder model needs to be trained on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITs3WTT5lXQT"
   },
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/log-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g9fhjxDlXQb"
   },
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L40ZxZW8lXQh"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install datasets>=2.6.1\n",
    "pip install farm-haystack[inference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJBcPNbBlXQq"
   },
   "source": [
    "### Enabling Telemetry\n",
    "\n",
    "Knowing you're using this tutorial helps us decide where to invest our efforts to build a better product but you can always opt out by commenting the following line. See [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUbTGVo4lXQv"
   },
   "outputs": [],
   "source": [
    "from haystack.telemetry import tutorial_running\n",
    "\n",
    "tutorial_running(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HLBUYOplXQ1"
   },
   "source": [
    "## Creating an Hybrid Retrieval Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usdANiAGlXQ9"
   },
   "source": [
    "### 1) Initialize the DocumentStore and Clean Documents\n",
    "\n",
    "You'll start creating a generative pipeline by initializing a DocumentStore, which will store the Documents to be chatted with.\n",
    "\n",
    "You will use the PubMed Abstracts as Documents. There are a lot of datasets from PubMed on Hugging Face Hub; you will use [this one](https://huggingface.co/datasets/ywchoi/pubmed_abstract_3/viewer/default/test) in this tutorial.\n",
    "\n",
    "Initialize `InMemoryDocumentStore` and don't forget to set `use_bm25=True` and the dimension of your embeddings in `embedding_dim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLbh-UtelXRL"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "dataset = load_dataset(\"ywchoi/pubmed_abstract_3\", split=\"test\")\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True, embedding_dim=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgxFjbGgdQla"
   },
   "source": [
    "You can create your document list with a simple for loop.\n",
    "The data has 3 features:\n",
    "* *pmid*\n",
    "* *title*\n",
    "* *text*\n",
    "\n",
    "We concatenate *title* and *text* to embed and search both. The single features are stored as metadata, and you will use them to have a **pretty print** of the search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvrG_QzirSsq"
   },
   "outputs": [],
   "source": [
    "from haystack.schema import Document\n",
    "\n",
    "documents = []\n",
    "for doc in dataset:\n",
    "    documents.append(\n",
    "        Document(\n",
    "            content=doc[\"title\"] + \" \" + doc[\"text\"],\n",
    "            meta={\"title\": doc[\"title\"], \"abstract\": doc[\"text\"], \"pmid\": doc[\"pmid\"]},\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNJkztzWaWzZ"
   },
   "source": [
    "The PreProcessor class is designed to help you clean and split text into sensible units.\n",
    "> To learn about the preprocessing step, check out [Tutorial: Preprocessing Your Documents](https://haystack.deepset.ai/tutorials/08_preprocessing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrCCmLvGqhYw"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=512,\n",
    "    split_overlap=32,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PzBU_jnsBTZ"
   },
   "outputs": [],
   "source": [
    "docs_to_index = preprocessor.process(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii9x0gr9lXRT"
   },
   "source": [
    "### 2) Initialize the Retrievers\n",
    "\n",
    "We initialize a sparse retriever using [BM25](https://docs.haystack.deepset.ai/docs/retriever#bm25-recommended) and a dense retriever using a [sentence-transformers model](https://docs.haystack.deepset.ai/docs/retriever#embedding-retrieval-recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXHbHru0lXRY"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import EmbeddingRetriever, BM25Retriever\n",
    "\n",
    "sparse_retriever = BM25Retriever(document_store=document_store)\n",
    "dense_retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    use_gpu=True,\n",
    "    scale_score=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cx8307ZglXRd"
   },
   "source": [
    "### 3) Write Documents ad Update Embeddings\n",
    "\n",
    "We write documents to the DocumentStore, first by deleting any remaining documents then calling `write_documents()`. The `update_embeddings()` method uses the retriever to create an embedding for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S-QdaDYlXRg"
   },
   "outputs": [],
   "source": [
    "document_store.delete_documents()\n",
    "document_store.write_documents(docs_to_index)\n",
    "document_store.update_embeddings(retriever=dense_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gugk_k2lXRi"
   },
   "source": [
    "### 4) Initialize JoinDocuments and Ranker\n",
    "\n",
    "While researching hybrid search, we needed a way to combine the results of BM25 and dense vector search into a single ranked list. It may not be obvious how to combine them:\n",
    "\n",
    "* Different retrievers use non-comparable score types, like BM25 and cosine similarity.\n",
    "* Documents may come from single or multiple sources at the same time. There should be a way to deal with duplicates in the final ranking.\n",
    "\n",
    "The merge and rank of the documents from different retrievers is an open problem, Haystack offers several methods in [`JoinDocuments`](https://docs.haystack.deepset.ai/docs/join_documents), here we use the simplest, `concatenate` and we postpone the task to the ranker.\n",
    "\n",
    "We use a [re-ranker based on a cross-encoder](https://docs.haystack.deepset.ai/docs/ranker#sentencetransformersranker) that scores the relevancy of all candidates for the given search query.\n",
    "For more information about the `Ranker` check the [docs](https://docs.haystack.deepset.ai/docs/ranker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_RiKspTlXRl"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import JoinDocuments, SentenceTransformersRanker\n",
    "\n",
    "join_documents = JoinDocuments(join_mode=\"concatenate\")\n",
    "rerank = SentenceTransformersRanker(model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PexSrsBLlXRp"
   },
   "source": [
    "## 5) Create an Hybrid Retrieval Pipeline\n",
    "\n",
    "With a Haystack `Pipeline` you can stick together your building blocks to a search pipeline. Under the hood, `Pipelines` are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases. To speed things up, Haystack also comes with a few predefined `Pipelines`.\n",
    "You can learn more about Pipelines in the [docs](https://docs.haystack.deepset.ai/docs/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0XLbnAXlXRt"
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_node(component=sparse_retriever, name=\"SparseRetriever\", inputs=[\"Query\"])\n",
    "pipeline.add_node(component=dense_retriever, name=\"DenseRetriever\", inputs=[\"Query\"])\n",
    "pipeline.add_node(component=join_documents, name=\"JoinDocuments\", inputs=[\"SparseRetriever\", \"DenseRetriever\"])\n",
    "pipeline.add_node(component=rerank, name=\"ReRanker\", inputs=[\"JoinDocuments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3bsFkHuhHn4"
   },
   "source": [
    "### Generating a Pipeline Diagram\n",
    "\n",
    "With any Pipeline, whether prebuilt or custom constructed, you can save a diagram showing how all the components are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCIMtwmThQG4"
   },
   "outputs": [],
   "source": [
    "# Uncomment the following to generate the images\n",
    "# !apt install libgraphviz-dev\n",
    "# !pip install pygraphviz\n",
    "\n",
    "# pipeline.draw(\"pipeline_hybrid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTTVLUJylXRx"
   },
   "source": [
    "Search and article with Hybrid Retrieval, if you want to see all the steps you can enable `debug=True` in `JoinDocuments`'s `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-5WbeBulXR0"
   },
   "outputs": [],
   "source": [
    "prediction = pipeline.run(\n",
    "    query=\"treatment for HIV\",\n",
    "    params={\n",
    "        \"SparseRetriever\": {\"top_k\": 10},\n",
    "        \"DenseRetriever\": {\"top_k\": 10},\n",
    "        \"JoinDocuments\": {\"top_k_join\": 15},  # comment for debug\n",
    "        # \"JoinDocuments\": {\"top_k_join\": 15, \"debug\":True}, #uncomment for debug\n",
    "        \"ReRanker\": {\"top_k\": 5},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvPv1cJ6gbBJ"
   },
   "source": [
    "Create a function to print a kind of *search page*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raL_z_sByDoQ"
   },
   "outputs": [],
   "source": [
    "def pretty_print_results(prediction):\n",
    "    for doc in prediction[\"documents\"]:\n",
    "        print(doc.meta[\"title\"], \"\\t\", doc.score)\n",
    "        print(doc.meta[\"abstract\"])\n",
    "        print(\"\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSUiizGNytwX"
   },
   "outputs": [],
   "source": [
    "pretty_print_results(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXxhhdQulXR5"
   },
   "source": [
    "## About us\n",
    "\n",
    "*Leave this section as is. It's a footer that we add to all our tutorials.*\n",
    "\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work:\n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://discord.com/invite/VBpFzsgRVF) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Haystack Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
