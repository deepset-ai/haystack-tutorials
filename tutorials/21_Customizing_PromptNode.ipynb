{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlTn1yekNONP"
   },
   "source": [
    "# Tutorial: Customizing PromptNode for NLP Tasks\n",
    "\n",
    "- **Level**: Intermediate\n",
    "- **Time to complete**: 20 minutes\n",
    "- **Nodes Used**: `PromptNode`, `PromptTemplate`\n",
    "- **Goal**: After completing this tutorial, you will have learned about how to use PromptNode and PromptTemplate for your custom NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffYFUAjUNONS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Learn how to summarize, categorize your documents and find a suitable title for them with large language models using PromptNode and PromptTemplate. In this tutorial, we'll use news from [The Guardian](https://www.theguardian.com/international) as documents but you can use any text you want.  \n",
    "\n",
    "This tutorial will introduce you to the basics of LLMs and PromptNode, showcase the pre-defined \"summarization\" template and explain how to use PromptTemplate to find titles for documents and categorize them with custom prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugkQ42iJNONS"
   },
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/log-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0rQfgOVNONS"
   },
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-00smnxNONT",
    "outputId": "5488121a-81f1-4591-acd7-89a1c9b2daf1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYtbl6qBNONU"
   },
   "source": [
    "## Using PromptNode as a Stand-Alone Node\n",
    "\n",
    "The PromptNode is the central abstraction in Haystack's large language model (LLM) support. It uses [`google/flan-t5-base`](https://huggingface.co/google/flan-t5-base) model by default, but you can replace the default model with a flan-t5 model of a different size such as `google/flan-t5-large` or a model by OpenAI such as `text-davinci-003`.\n",
    "\n",
    "Large language models are huge models trained on enormous amounts of data. That’s why these models have general knowledge of the world, so you can ask them anything and they will be able to answer. Let's initialize a PromptNode and see how we can prompt large language models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wYbU8WhNX08"
   },
   "source": [
    "1. Initialize a PromptNode instance. For this tutorial, we'll use [`google/flan-t5-large`](https://huggingface.co/google/flan-t5-large) as our large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wa-I31YZNONU"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes.prompt import PromptNode\n",
    "\n",
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE7s-cMRNh5f"
   },
   "source": [
    "> If you want to use PromptNode with an OpenAI model, change the model name and provide an `api_key`. \n",
    "> ```python\n",
    "> prompt_node = PromptNode(model_name_or_path=\"text-davinci-003\", api_key=<YOUR_API_KEY>)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO5z2H_mO8cg"
   },
   "source": [
    "2. Ask a question. `google/flan-t5-large` can answer general questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBxL4odqNg6K",
    "outputId": "2e12c341-b24d-45e7-f758-a93e81cdc570"
   },
   "outputs": [],
   "source": [
    "prompt_node(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cg-8tKebDCa0",
    "outputId": "e108ad6d-5e44-4108-e308-4c85da2b1951"
   },
   "outputs": [],
   "source": [
    "prompt_node(\"What is the highest mountain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MVeOCWSNONV"
   },
   "source": [
    "3. The `google/flan-t5-large` was trained on school math word problems dataset named [GSM8K](https://huggingface.co/datasets/gsm8k). That's why this model can answer basic math questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZpTIQEUNONW",
    "outputId": "fe97895b-c475-4de2-ad70-858201f008ad"
   },
   "outputs": [],
   "source": [
    "prompt_node(\"If Bob is 20 and Sara is 11, who is older?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aybEtTGIQQSX"
   },
   "source": [
    "Let's see how we can use PromptNode for more advanced tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6hK9AcsNONW"
   },
   "source": [
    "## Using PromptNode With a Template\n",
    "\n",
    "PromptNode comes with out-of-the-box prompt templates to quickly interact with the large language model. These templates can perform multiple tasks, such as summarization, question answering, question generation, and more, using a single, unified model within the Haystack framework. To use these templates, just provide the documents and additional necessary information  the PromptNode. \n",
    "\n",
    "Let's see how we can use PromptNode to generate summary of news.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zECQSKkWDYns"
   },
   "source": [
    "1. Define news to use as `documents` for the PromptNode. We'll use these documents for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjYmEk9_dyHZ"
   },
   "outputs": [],
   "source": [
    "from haystack.schema import Document\n",
    "\n",
    "# https://www.theguardian.com/business/2023/feb/12/inflation-may-have-peaked-but-the-cost-of-living-pain-is-far-from-over\n",
    "news_economics = Document(\n",
    "    \"\"\"At long last, Britain’s annual inflation rate is on the way down. After hitting the highest level since the 1980s, heaping pressure on millions of households as living costs soared, official figures this week could bring some rare good news.\n",
    "City economists expect UK inflation to have cooled for a third month running in January – the exact number is announced on Wednesday – helped by falling petrol prices and a broader decline in the global price of oil and gas in recent months. The hope now is for a sustained decline in the months ahead, continuing a steady drop from the peak of 11.1% seen in October.\n",
    "The message from the Bank of England has been clear. Inflation is on track for a “rapid” decline over the coming months, raising hopes that the worst of Britain’s cost of living crisis is now in the rearview mirror.\n",
    "There are two good reasons for this. Energy costs are moving in the right direction, while the initial rise in wholesale oil and gas prices that followed Russia’s invasion of Ukraine in February last year will soon drop from the calculation of the annual inflation rate.\"\"\"\n",
    ")\n",
    "\n",
    "# https://www.theguardian.com/science/2023/feb/13/starwatch-orions-belt-and-sirius-lead-way-to-hydras-head\n",
    "news_science = Document(\n",
    "    \"\"\"On northern winter nights, it is so easy to be beguiled by the gloriously bright constellations of Orion, the hunter, and Taurus, the bull, that one can overlook the fainter constellations.\n",
    "So this week, find the three stars of Orion’s belt, follow them down to Sirius, the brightest star in the night sky, and then look eastward until you find the faint ring of stars that makes up the head of Hydra, the water snake. The chart shows the view looking south-east from London at 8pm GMT on Monday, but the view will be similar every night this week.\n",
    "Hydra is the largest of the 88 modern constellations covering an area of 1,303 square degrees. To compare, nearby Orion only covers 594 square degrees. Hydra accounts for most of its area by its length, crossing more than 100 degrees of the sky (the full moon spans half a degree).\n",
    "As evening becomes night and into the early hours, the rotation of Earth causes Hydra to slither its way across the southern meridian until dawn washes it from the sky. From the southern hemisphere, the constellation is easily visible in the eastern sky by mid-evening.\"\"\"\n",
    ")\n",
    "\n",
    "# https://www.theguardian.com/music/2023/jan/30/salisbury-cathedral-pipe-organ-new-life-holst-the-planets\n",
    "news_culture = Document(\n",
    "    \"\"\"A unique performance of Gustav Holst’s masterwork The Planets – played on a magnificent pipe organ rather than by an orchestra and punctuated by poems inspired by children’s responses to the music – is to be staged in the suitably vast Salisbury Cathedral.\n",
    "The idea of the community music project is to introduce more people, young and old, to the 140-year-old “Father” Willis organ, one of the treasures of the cathedral.\n",
    "It is also intended to get the children who took part and the adults who will watch and listen thinking afresh about the themes Holst’s suite tackles – war, peace, joy and mysticism – which seem as relevant now as when he wrote the work a century ago.\n",
    "John Challenger, the cathedral’s principal organist, said: “We have a fantastic pipe organ largely as it was when built. It’s a thrilling thing. I view it as my purpose in life to share it with as many people as possible.”\n",
    "The Planets is written for a large orchestra. “Holst calls for huge instrumental forces and an unseen distant choir of sopranos and altos,” said Challenger. But he has transposed the suite for the organ, not copying the effect of the orchestral instruments but finding a new version of the suite.\"\"\"\n",
    ")\n",
    "\n",
    "# https://www.theguardian.com/sport/blog/2023/feb/14/multi-million-dollar-wpl-auction-signals-huge-step-forward-for-womens-sport\n",
    "news_sport = Document(\n",
    "    \"\"\"It was only a few days ago that members of the Australian women’s cricket team were contemplating how best to navigate the impending “distraction” of the inaugural Women’s Premier League auction, scheduled during the first week of the T20 World Cup. “It’s a little bit awkward,” captain Meg Lanning said in South Africa last week. “But it’s just trying to embrace that and understanding it’s actually a really exciting time and you actually don’t have a lot of control over most of it, so you’ve just got to wait and see.”\n",
    "What a pleasant distraction it turned out to be. Lanning herself will be $192,000 richer for three weeks’ work with the Delhi Capitals. Her teammate, Ash Gardner, will earn three times that playing for the Gujarat Giants. The allrounder’s figure of $558,000 is more than Sam Kerr pockets in a season with Chelsea and more than the WNBA’s top earner, Jackie Young.\n",
    "If that sounds like a watershed moment, it’s perhaps because it is. And it is not the only one this past week. The NRLW made its own wage-related headlines on Tuesday, to the effect that the next (agreed in principle) collective bargaining agreement will bring with it a $1.5m salary cap in 2027, at an average salary of $62,500. Women’s rugby, too, is making moves, with news on the weekend that Rugby Australia will begin contracting the Wallaroos.\"\"\"\n",
    ")\n",
    "\n",
    "news = [news_economics, news_science, news_culture, news_sport]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ezq9NVmFdzz1"
   },
   "source": [
    "> The token limit for `google/flan-t5-large` is 512. So, all news pieces should be shorter than the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okw4m5aeL7hy"
   },
   "source": [
    "2. List pre-defined templates using `get_prompt_template_names()` method. All templates come with necessary prompts to perform these tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqwLmdePNONW",
    "outputId": "b33d9123-9cf3-4444-fc04-1bc8311e7338"
   },
   "outputs": [],
   "source": [
    "prompt_node.get_prompt_template_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syB5rl3xIgHr"
   },
   "source": [
    "3. Use `summarization` template and generate a summary for each piece of news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ygxFMwb4NONX",
    "outputId": "e56ed89a-b020-4f00-9310-b1a643cb87ea"
   },
   "outputs": [],
   "source": [
    "prompt_node.prompt(prompt_template=\"summarization\", documents=news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUTckmA1PzLc"
   },
   "source": [
    "Now you know how to use PromptNode. Let's see how we can create a custom template for other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0vQ45tHNONX"
   },
   "source": [
    "## Using PromptNode with a Custom Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "992kb_WKNONX"
   },
   "source": [
    "The biggest benefit of PromptNode is that you can use it to define and add additional prompt templates. Defining additional prompt templates makes it possible to extend the model's capabilities and use it for a broader range of NLP tasks in Haystack. \n",
    "\n",
    "You can define custom templates for each NLP task and register them with PromptNode. Let's create a custom template to generate descriptive titles for news:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en6IbPRsTOTz"
   },
   "source": [
    "1. Initialize a `PromptTemplate` instance. We need `name` parameter to refer to the prompt template and `prompt_text` parameter to define the prompt. We include the parameters for the prompt in `promp_text` with a `$` sign in front of the parameter name. For the new `give-a-title` template, we only need `$news` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiOiya2UV4WW"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes.prompt import PromptTemplate\n",
    "\n",
    "title_generator = PromptTemplate(\n",
    "    name=\"give-a-title\",\n",
    "    prompt_text=\"Provide a short, descriptive title for the given piece of news. News: $news; Title:\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jttb_ROnWYwS"
   },
   "source": [
    "2. To use the new template, pass the new `title_generator` as the `prompt_template` to the `prompt()` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zX_nALriWXxx",
    "outputId": "b027b36c-ef1f-4c5b-c392-7bc9d6364838"
   },
   "outputs": [],
   "source": [
    "prompt_node.prompt(prompt_template=title_generator, news=news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0Mx3lHyXK_I"
   },
   "source": [
    "> If you add a custom template to the template list, call `add_prompt_template()` with the `PromptTemplate` object and you can start using the template only with its `name`. \n",
    "> ```python\n",
    "> prompt_node.add_prompt_template(PromptTemplate(name=\"give-a-title\", prompt_text=\"Provide a short, descriptive title for the given piece of news. News: $news; Title:\"))\n",
    "> prompt_node.prompt(prompt_template=\"give-a-title\", news=news)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH4QbygONONX"
   },
   "source": [
    "You can customize PromptTemplates as much as you want according to your needs Let's try to categorize the news and see how you can customize the prompt further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAELXO0MhbrV"
   },
   "source": [
    "1. Create another PromptTemplate called `categorize-news`. In the `prompt_text`, define `$news` parameter, give categories and ask model not to categorize the news if it doesn't fit in the provided category list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHgOjTwkNONY"
   },
   "outputs": [],
   "source": [
    "news_categorizer = PromptTemplate(\n",
    "    name=\"categorize-news\",\n",
    "    prompt_text=\"Given the categories: sport, economics, culture; classify the news: $news. Only pick a category from the list, otherwise say: no suitable category\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC4gVHCyuXHl"
   },
   "source": [
    "2. Run the `prompt()` method with `news_categorizer` template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taktlLIfNONY",
    "outputId": "1e9a5947-1074-47a6-849b-3b55992983b2"
   },
   "outputs": [],
   "source": [
    "prompt_node.prompt(prompt_template=news_categorizer, news=news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zujY1e-XH0VQ"
   },
   "source": [
    "And that's it! Now you know how to use PromptNode and create custom prompts with PromptTemplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKsx2KbNONY"
   },
   "source": [
    "## About us\n",
    "\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76c0ba09435b0d7ab7f3e4f9fd6b1554cdb12adf349a7242a1470606c432d777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
