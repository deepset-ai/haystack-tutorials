{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Using Haystack with REST API\n",
    "\n",
    "- **Level**: Advanced\n",
    "- **Time to complete**: 30 minutes\n",
    "- **Prerequisites**: Basic understanding of Docker, basic knowledge of Haystack pipelines \n",
    "- **Nodes Used**: `ElasticsearchDocumentStore`, `EmbeddingRetriever`\n",
    "- **Goal**: Learn how you can interact with Haystack through REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Haystack enables you to apply the latest NLP technology to your own data and create production-ready applications. Building an end-to-end NLP application requires the combination of multiple concepts. Here are those concepts:\n",
    "* **DocumentStore** stores the data. You will use Elasticsearch for this tutorial.\n",
    "* **Haystack** pipelines convert files into [Documents](https://docs.haystack.deepset.ai/docs/documents_answers_labels#document), index them to DocumentStore, and run NLP tasks such as question answering and document search.\n",
    "* **REST API** allows other applications to interact with Haystack by handling their queries and returning responses.\n",
    "* **Docker** simplifies the environment set-up needed to have Elasticsearch running.\n",
    "\n",
    "This tutorial introduces you to all the concepts needed to build an end-to-end document search application. After completing this tutorial, you will have learned how to create a pipeline YAML file, index files and query your application using REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Update or install Docker and Docker Compose, and launch Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "apt-get update && apt-get install docker && apt-get install docker-compose\n",
    "service docker start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install Haystack.\n",
    "\n",
    "Install the latest version of Haystack from the main branch and all its dependencies. REST API and `xpdf` are also required.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install 'farm-haystack[all]'\n",
    "pip install -e rest_api/\n",
    "\n",
    "brew install xpdf # required for PDFToTextConverter node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Clone Haystack repository.\n",
    "\n",
    "Haystack provides a `docker-compose.yml` file that defines a container for Elasticsearch. Clone the Haystack repository to be able to run the `docker-compose.yml` file locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/deepset-ai/haystack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Update the `docker-compose.yml` file.\n",
    "\n",
    "The current `docker-compose.yml` file has basic settings and it also provides an image for Elasticsearch container which needs to change. For that, go to `docker-compose.yml` and replace `image: \"deepset/elasticsearch-countries-and-capitals\"` with `image: \"docker.elastic.co/elasticsearch/elasticsearch:7.9.2\"`. The new image is going to provide an empty Elasticsearch instance instead of an instance with some indexed articles about countries and capital cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```yaml\n",
    "services:\n",
    "  ...\n",
    "  elasticsearch:\n",
    "    image: \"docker.elastic.co/elasticsearch/elasticsearch:7.9.2\"\n",
    "    ports:\n",
    "      - 9200:9200\n",
    "    restart: on-failure\n",
    "    environment:\n",
    "      - discovery.type=single-node\n",
    "      - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\"\n",
    "    healthcheck:\n",
    "        test: curl --fail http://localhost:9200/_cat/health || exit 1\n",
    "        interval: 10s\n",
    "        timeout: 1s\n",
    "        retries: 10\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Launch Elasticsearch.\n",
    "\n",
    "Go to the directory where `docker-compose.yml` is and start the Elasticsearch container. You might have realised that there are settings for haystack and ui containers provided in the `docker-compose.yml`. They are not necessary for in this tutorial. Run `docker-compose up` with the `elasticsearch` argument so that only the Elasticsearch container launches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd haystack\n",
    "docker-compose up elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Check Elasticsearch health.\n",
    "\n",
    "Launching Elasticsearch takes some time, so, make sure you have a `healthy` Elasticsearch container before continuing. You can check the container health with the `docker ps` command. A `healthy` container should have a healthy status on port 9200.\n",
    "\n",
    "![command line output of `docker ps`](https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/img/tutorial20_elasticsearch_healthy.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline YAML File\n",
    "\n",
    "YAML files are widely used for configurations. Haystack enables defining pipelines as YAML files and the `load_from_yaml()` method loads pipelines from YAML file. In a YAML file, the `components` section defines all pipeline nodes and `pipelines` section defines how these nodes are connected to each other to form a pipeline. Let's start with defining query and indexing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a document search pipeline.\n",
    "\n",
    "To act as your query pipeline, you will design a document search pipeline from scratch. Create a new file named `document-search.haystack-pipeline.yml` in `/pipeline` folder under `/rest_api` in the Haystack code base. Then, update the `PIPELINE_YAML_PATH` value in `rest_api/config.py` with the new file name. `PIPELINE_YAML_PATH` variable will tell the REST API which YAML file to run. \n",
    "\n",
    "```python\n",
    "PIPELINE_YAML_PATH = os.getenv(\n",
    "    \"PIPELINE_YAML_PATH\", str((Path(__file__).parent / \"pipeline\" / \"document-search.haystack-pipeline.yml\").absolute())\n",
    ")\n",
    "```\n",
    "\n",
    "A document search pipeline requires a DocumentStore and a Retriever. Use `ElasticsearchDocumentStore` and `EmbeddingRetriever` for these nodes respectively and define them under `components`. For each component, `type` needs to refer to a node class in Haystack, `name` refers how these nodes are called in the pipeline YAML, and `params` is used for the parameters that can be provided to the node. As Retriever parameters, provide `document_store`, `embedding_model` and a `top_k` value.\n",
    "\n",
    "```yaml\n",
    "components:\n",
    "  - name: DocumentStore\n",
    "    type: ElasticsearchDocumentStore\n",
    "  - name: Retriever\n",
    "    type: EmbeddingRetriever\n",
    "    params:\n",
    "      document_store: DocumentStore\n",
    "      top_k: 5 \n",
    "      embedding_model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "```\n",
    "\n",
    "After defining components, create a query pipeline in `pipelines` section. Here, `name` refers to the name of the pipeline and `nodes` defines how the pipeline is built. \n",
    "\n",
    "```yaml\n",
    "pipelines:\n",
    "  - name: query \n",
    "    nodes:\n",
    "      - name: Retriever\n",
    "        inputs: [Query]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create an indexing pipeline.\n",
    "\n",
    "You can define an indexing pipeline in the same pipeline YAML file and index your documents to Elasticsearch through REST API. For that, create `FileTypeClassifier`, `TextConverter`, `PDFToTextConverter`, and last, `PreProcessor` nodes with parameters to split documents. \n",
    "\n",
    "```yaml\n",
    "components:\n",
    "    ...\n",
    "  - name: FileTypeClassifier\n",
    "    type: FileTypeClassifier\n",
    "  - name: TextFileConverter\n",
    "    type: TextConverter\n",
    "  - name: PDFFileConverter\n",
    "    type: PDFToTextConverter\n",
    "  - name: Preprocessor\n",
    "    type: PreProcessor\n",
    "    params:\n",
    "      split_by: word\n",
    "      split_length: 1000\n",
    "      split_overlap: 50 \n",
    "      split_respect_sentence_boundary: True \n",
    "```\n",
    "\n",
    "Then, go below to the `pipelines` section of the YAML file and create a new pipeline called `indexing`. In this pipeline, indicate how these nodes are connected to each other, Retriever, and DocumentStore. This indexing pipeline supports `.txt` and `.pdf` files and pre-processes them before loading to the Elasticsearch.\n",
    "\n",
    "```yaml\n",
    "pipelines:\n",
    "  ...\n",
    "  - name: indexing\n",
    "    nodes:\n",
    "      - name: FileTypeClassifier\n",
    "        inputs: [File]\n",
    "      - name: TextFileConverter\n",
    "        inputs: [FileTypeClassifier.output_1]\n",
    "      - name: PDFFileConverter\n",
    "        inputs: [FileTypeClassifier.output_2]\n",
    "      - name: Preprocessor\n",
    "        inputs: [PDFFileConverter, TextFileConverter]\n",
    "      - name: Retriever\n",
    "        inputs: [Preprocessor]\n",
    "      - name: DocumentStore\n",
    "        inputs: [Retriever]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing query and indexing pipelines, add `version: ignore` to the top of the file. Now, the pipeline YAML is completed.\n",
    "\n",
    "```yaml\n",
    "version: ignore\n",
    "\n",
    "components:\n",
    "  - name: DocumentStore\n",
    "    type: ElasticsearchDocumentStore\n",
    "  - name: Retriever\n",
    "    type: EmbeddingRetriever\n",
    "    params:\n",
    "      document_store: DocumentStore\n",
    "      top_k: 5 \n",
    "      embedding_model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "  - name: FileTypeClassifier\n",
    "    type: FileTypeClassifier\n",
    "  - name: TextFileConverter\n",
    "    type: TextConverter\n",
    "  - name: PDFFileConverter\n",
    "    type: PDFToTextConverter\n",
    "  - name: Preprocessor\n",
    "    type: PreProcessor\n",
    "    params:\n",
    "      split_by: word\n",
    "      split_length: 1000\n",
    "      split_overlap: 50 \n",
    "      split_respect_sentence_boundary: True\n",
    "\n",
    "pipelines:\n",
    "  - name: query \n",
    "    nodes:\n",
    "      - name: Retriever\n",
    "        inputs: [Query]\n",
    "  - name: indexing\n",
    "    nodes:\n",
    "      - name: FileTypeClassifier\n",
    "        inputs: [File]\n",
    "      - name: TextFileConverter\n",
    "        inputs: [FileTypeClassifier.output_1]\n",
    "      - name: PDFFileConverter\n",
    "        inputs: [FileTypeClassifier.output_2]\n",
    "      - name: Preprocessor\n",
    "        inputs: [PDFFileConverter, TextFileConverter]\n",
    "      - name: Retriever\n",
    "        inputs: [Preprocessor]\n",
    "      - name: DocumentStore\n",
    "        inputs: [Retriever]\n",
    "```\n",
    "\n",
    "Feel free to play with the pipeline setup later on. Add or remove some nodes, change the parameters or add new ones. Check out [Haystack API Reference](https://docs.haystack.deepset.ai/reference/answer-generator-api) for more options on nodes and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Haystack API\n",
    "\n",
    "Now, you can start the REST API server running the pipelines above with gunicorn server. When the startup is complete, you will see it running on port 8000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gunicorn rest_api.application:app -b 0.0.0.0:8000 -k uvicorn.workers.UvicornWorker -t 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, test if everything is okay with the Haystack API by sending a cURL request to the `/initialized` endpoint. You can use command line of your computer or tools like [Postman](https://www.postman.com/) to send cURL requests. If there is no problem, you will get a response as `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl --request GET http://127.0.0.1:8000/initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Files to Elasticsearch\n",
    "\n",
    "Right now, the Elasticsearch instance is empty. Haystack API provides a `/file-upload` endpoint to upload files to Elasticsearch using the indexing pipeline defined in the pipeline YAML. After indexing files to Elasticsearch, you will be able to perform document search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download example files.\n",
    "\n",
    "Download the [example files](https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/article_txt_countries_and_capitals.zip) you will be indexing to Elasticsearch. In the folder, there are text files about countries and capitals crawled from [Wikipedia](https://en.wikipedia.org/wiki/Category:Lists_of_countries_by_continent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Index files to Elasticsearch.\n",
    "\n",
    "You can send cURL requests to the `/file-upload` endpoint to upload files to the Elasticsearch instance. Replace `<PATH_TO_FOLDER>` with the path to the example files on your computer and send the POST request. If the file is successfully uploaded, you will get a response as `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl --request POST \\\n",
    "     --url http://127.0.0.1:8000/file-upload \\\n",
    "     --header 'accept: application/json' \\\n",
    "     --header 'content-type: multipart/form-data' \\\n",
    "     --form files=@<PATH_TO_FOLDER>0_Minsk.txt \\\n",
    "     --form meta=null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this method is not convenient for uploading multiple files to the Elasticsearch instance as replacing the file names in the request by hand is difficult. Instead, create a python file in the folder that you keep the example files, put the code below into the python file, and run the python script. This python code takes the name of every file in the folder and sends the POST request to the url `http://127.0.0.1:8000/file-upload`. With the `print(response.text, \":\", file_name)` line, you will be able to see the names of all indexed files. Make sure you see the \"Completed\" text before continuing. \n",
    "\n",
    "```python\n",
    "import os\n",
    "import requests\n",
    " \n",
    "file_list = os.listdir()\n",
    "url = \"http://127.0.0.1:8000/file-upload\"\n",
    "payload = {\"meta\": \"null\"}\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "for file_name in file_list:\n",
    "    files = {\"files\": (file_name, open(file_name, \"rb\"), \"text/plain\")}\n",
    "    response = requests.post(url, data=payload, files=files, headers=headers)\n",
    "    print(response.text, \":\", file_name)\n",
    "\n",
    "print(\"Completed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voil√†! Make a query!\n",
    "\n",
    "The application is ready! Send a cURL request to retrieve documents about _\"climate in Scandinavia\"_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl --request POST \\\n",
    "     --url http://127.0.0.1:8000/query \\\n",
    "     --header 'accept: application/json' \\\n",
    "     --header 'content-type: application/json' \\\n",
    "     --data '{\n",
    "     \"query\": \"climate in Scandinavia\"\n",
    "     }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As response, you will get a `QueryResponse` object consists of `query`, `answers`, and `documents`. Related Documents to your query will be under the `documents` attribute of the object.\n",
    "\n",
    " ```bash\n",
    "{\n",
    "  \"query\": \"climate in Scandinavia\",\n",
    "  \"answers\": [],\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": \"52937ad257317032b9aed9750b5fcbb7\",\n",
    "      \"content\": \"Even though temperature patterns differ between north and south, the summer climate is surprisingly similar all through the entire country in spite of the large latitudinal differences. This is due to the south's being surrounded by a greater mass of water, with the wider Baltic Sea and the Atlantic air passing over lowland areas from the south-west. ...\",\n",
    "      \"content_type\": \"text\",\n",
    "      \"meta\": {\n",
    "        \"_split_id\": 7,\n",
    "        \"name\": \"43_Sweden.txt\"\n",
    "      },\n",
    "      \"score\": 0.569930352925387\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) tutorial was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
