{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Creating a Generative QA Pipeline with PromptNode\n",
    "\n",
    "- **Level**: Advanced\n",
    "- **Time to complete**: 15 minutes\n",
    "- **Nodes Used**: `InMemoryDocumentStore`, `BM25Retriever`, `PromptNode`, `PromptTemplate` `Shaper`\n",
    "- **Goal**: After completing this tutorial, you'll have created a generative question answering search system that uses a large language model through PromptNode with the help of Shaper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Learn how to build a generative question answering pipeline using the power of LLMs with PromptNode. In this tutorial, we'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents, but you can replace them with any text you want. \n",
    "\n",
    "This tutorial introduces you to the new Shaper node and explains how to use Shaper to integrate PromptNode in the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/log-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab]\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Telemetry\n",
    "\n",
    "Knowing you're using this tutorial helps us decide where to invest our efforts to build a better product but you can always opt out by commenting the following line. See [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.telemetry import tutorial_running\n",
    "\n",
    "tutorial_running(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the DocumentStore\n",
    "\n",
    "We'll start creating our question answering system by initializing a DocumentStore. A DocumentStore stores the Documents that the question answering system uses to find answers to your questions. In this tutorial, we're using the `InMemoryDocumentStore`.\n",
    "\n",
    "Let's initialize our DocumentStore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `InMemoryDocumentStore` is the simplest DocumentStore to get started with. It requires no external dependencies and it's a good option for smaller projects and debugging. But it doesn't scale up so well to larger Document collections, so it's not a good choice for production systems. To learn more about the DocumentStore and the different types of external databases that we support, see [DocumentStore](https://docs.haystack.deepset.ai/docs/document_store)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DocumentStore is now ready. Now it's time to fill it with some Documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching and Writing Documents\n",
    "\n",
    "We'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents. We preprocessed the data and uploaded to a Hugging Face Space: [Seven Wonders](https://huggingface.co/datasets/bilgeyucel/seven-wonders). Thus, we don't need to perform any additional cleaning or splitting. \n",
    "\n",
    "Let's fetch the data and write it to the DocumentStore: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "\n",
    "document_store.write_documents(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Retriever\n",
    "\n",
    "Let's initialize a BM25Retriever and make it use the InMemoryDocumentStore we initialized earlier in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the PromptNode\n",
    "\n",
    "Let's define a custom prompt to use with our PromptNode. This prompt will accept `$texts` and `$query` as parameters. `$text` will match the output of the Shaper and `$query` will match the `query` we pass at runtime. \n",
    "\n",
    "We'll initialize PromptNode with the new PromptTemplate and `google/flan-t5-large` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate\n",
    "\n",
    "lfqa_prompt = PromptTemplate(\n",
    "    name=\"lfqa\",\n",
    "    prompt_text=\"\"\"Synthesize a comprehensive answer from the following text for the given question. \n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text. \n",
    "                             Your answer should be in your own words and be no longer than 50 words. \n",
    "                             \\n\\n Related text: $texts \\n\\n Question: $query \\n\\n Answer:\"\"\",\n",
    ")\n",
    "\n",
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=lfqa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To learn about how to use custom templates with PromptNode, check out [Customizing PromptNode for NLP Tasks](https://haystack.deepset.ai/tutorials/21_customizing_promptnode) tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Shaper\n",
    "\n",
    "[Shaper](https://docs.haystack.deepset.ai/docs/shaper) is necessary when the output of one node does not match what the next node expects as input. In our pipeline, we need to join retrieved Documents so that we can inject these Documents into the prompt. We can solve this problem by defining a Shaper that uses [`join_documents`](https://docs.haystack.deepset.ai/reference/shaper-api#join_documents) as its function (`func`). Retriever refers Documents as `documents` and `join_documents` expects `documents` parameter, so, we can pass `{\"documents\": \"documents\"}` to the Shaper as `inputs`. To output joined Documents as `texts`, we need to define `outputs=[\"texts\"]`.    \n",
    "\n",
    "Let's initialize the Shaper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import Shaper\n",
    "\n",
    "shaper = Shaper(func=\"join_documents\", inputs={\"documents\": \"documents\"}, outputs=[\"texts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Pipeline\n",
    "\n",
    "We'll use a custom pipeline with the Retriever, Shaper, and PromptNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=shaper, name=\"shaper\", inputs=[\"retriever\"])\n",
    "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"shaper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! The pipeline's ready to generate answers to questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking a Question\n",
    "\n",
    "We use the pipeline `run()` method to ask a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe.run(query=\"How does Rhodes Statue look like?\")\n",
    "\n",
    "print(output[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some other example queries to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Where is Gardens of Babylon?\",\n",
    "    \"Why did people build Great Pyramid of Giza?\",\n",
    "    \"How does Rhodes Statue look like?\",\n",
    "    \"Why did people visit the Temple of Artemis?\",\n",
    "    \"What is the importance of Colossus of Rhodes?\",\n",
    "    \"What happened to the Tomb of Mausolus?\",\n",
    "    \"How did Colossus of Rhodes collapse?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ‰ Congratulations! You've learned how to create a generative QA system for your documents with PromptNode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
