{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dne2XSNzB3SK"
   },
   "source": [
    "# Make Your QA Pipelines Talk!\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://upload.wikimedia.org/wikipedia/en/d/d8/Game_of_Thrones_title_card.jpg\">\n",
    "\n",
    "Question answering works primarily on text, but Haystack provides some features for audio files that contain speech as well.\n",
    "\n",
    "In this tutorial, we're going to see how to use `AnswerToSpeech` to convert answers into audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7sF5gZnPgnG"
   },
   "source": [
    ">*Update:* AnswerToSpeech lives in the [text2speech](https://github.com/deepset-ai/haystack-extras/tree/main/nodes/text2speech) package. Main [Haystack](https://github.com/deepset-ai/haystack) reposity doesn't include it anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4UBjfz4LB3SS"
   },
   "source": [
    "### Prepare environment\n",
    "\n",
    "#### Colab: Enable the GPU runtime\n",
    "Make sure you enable the GPU runtime to experience decent speed in this tutorial.\n",
    "**Runtime -> Change Runtime type -> Hardware accelerator -> GPU**\n",
    "\n",
    "<img src=\"https://github.com/deepset-ai/haystack-tutorials/raw/main/tutorials/img/colab_gpu_runtime.jpg\">\n",
    "\n",
    "You can double check whether the GPU runtime is enabled with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDHmaD2gB3SX",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBvGUPVKN2oJ"
   },
   "source": [
    "To start, let's install the latest release of Haystack with `pip`. In this tutorial, we'll use components from [text2speech](https://github.com/deepset-ai/haystack-extras/tree/main/nodes/text2speech) which contains some extra Haystack components, so we'll install `farm-haystack-text2speech`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsY0HC8JB3Sc",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab]\n",
    "pip install farm-haystack-text2speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "r3kPhIxKN2oK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logging\n",
    "\n",
    "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
    "Example log message:\n",
    "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
    "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DyCNTb-zN2oK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbGu92rAB3Sl"
   },
   "source": [
    "## Indexing Documents\n",
    "\n",
    "First of all, we will populate the document store with a simple indexing pipeline. See [Tutorial 1](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb) for more details about these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWYnP3nWB3So",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import FileTypeClassifier, TextConverter, PreProcessor\n",
    "\n",
    "# Initialize the DocumentStore\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "\n",
    "# Get the documents\n",
    "documents_path = \"data/tutorial17\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt17.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=documents_path)\n",
    "\n",
    "# List all the paths\n",
    "file_paths = [p for p in Path(documents_path).glob(\"**/*\")]\n",
    "\n",
    "# NOTE: In this example we're going to use only one text file from the wiki\n",
    "file_paths = [p for p in file_paths if \"Stormborn\" in p.name]\n",
    "\n",
    "# Prepare some basic metadata for the files\n",
    "files_metadata = [{\"name\": path.name} for path in file_paths]\n",
    "\n",
    "# Makes sure the file is a TXT file (FileTypeClassifier node)\n",
    "classifier = FileTypeClassifier()\n",
    "\n",
    "# Converts a file into text and performs basic cleaning (TextConverter node)\n",
    "text_converter = TextConverter(remove_numeric_tables=True)\n",
    "\n",
    "# - Pre-processes the text by performing splits and adding metadata to the text (Preprocessor node)\n",
    "preprocessor = PreProcessor(clean_header_footer=True, split_length=200, split_overlap=20)\n",
    "\n",
    "# Here we create a basic indexing pipeline\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_node(classifier, name=\"classifier\", inputs=[\"File\"])\n",
    "indexing_pipeline.add_node(text_converter, name=\"text_converter\", inputs=[\"classifier.output_1\"])\n",
    "indexing_pipeline.add_node(preprocessor, name=\"preprocessor\", inputs=[\"text_converter\"])\n",
    "indexing_pipeline.add_node(document_store, name=\"document_store\", inputs=[\"preprocessor\"])\n",
    "\n",
    "# Then we run it with the documents and their metadata as input\n",
    "output = indexing_pipeline.run(file_paths=file_paths, meta=files_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zW5qaqn1B3St"
   },
   "source": [
    "### Querying\n",
    "   \n",
    "Now we will create a pipeline very similar to the basic `ExtractiveQAPipeline` of Tutorial 1,\n",
    "with the addition of a node that converts our answers into audio files! Once the answer is retrieved, we can also listen to the audio version of the document where the answer came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_oecui1B3Sw"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import BM25Retriever, FARMReader\n",
    "from text2speech import AnswerToSpeech\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "answer2speech = AnswerToSpeech(\n",
    "    model_name_or_path=\"espnet/kan-bayashi_ljspeech_vits\", generated_audio_dir=Path(\"./audio_answers\")\n",
    ")\n",
    "\n",
    "audio_pipeline = Pipeline()\n",
    "audio_pipeline.add_node(retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "audio_pipeline.add_node(reader, name=\"Reader\", inputs=[\"Retriever\"])\n",
    "audio_pipeline.add_node(answer2speech, name=\"AnswerToSpeech\", inputs=[\"Reader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV1KHzXGB3Sy"
   },
   "source": [
    "## Ask a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-ZMUBzpB3Sz",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# You can configure how many candidates the Reader and Retriever shall return\n",
    "# The higher top_k_retriever, the better (but also the slower) your answers.\n",
    "prediction = audio_pipeline.run(\n",
    "    query=\"Who is the father of Arya Stark?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpFSxtNNB3S1"
   },
   "outputs": [],
   "source": [
    "# Now you can print prediction\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xg6BN4v8N2oM"
   },
   "outputs": [],
   "source": [
    "# The document the first answer was extracted from\n",
    "original_document = [doc for doc in prediction[\"documents\"] if doc.id == prediction[\"answers\"][0].document_ids[0]][0]\n",
    "pprint(original_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXf-kTn4B3S6"
   },
   "source": [
    "## Hear them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cJJVpT7dB3S7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRIotUuz-mpF"
   },
   "outputs": [],
   "source": [
    "prediction[\"answers\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usGVf1N6B3S8"
   },
   "outputs": [],
   "source": [
    "# The first answer in isolation\n",
    "\n",
    "print(\"Answer: \", prediction[\"answers\"][0].meta[\"answer_text\"])\n",
    "\n",
    "speech, _ = sf.read(prediction[\"answers\"][0].answer)\n",
    "display(Audio(speech, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTFwNJqtB3S9"
   },
   "outputs": [],
   "source": [
    "# The context of the first answer\n",
    "\n",
    "print(\"Context: \", prediction[\"answers\"][0].meta[\"context_text\"])\n",
    "\n",
    "speech, _ = sf.read(prediction[\"answers\"][0].context)\n",
    "display(Audio(speech, rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wJpoQQNdB3S-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
