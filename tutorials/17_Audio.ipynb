{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dne2XSNzB3SK"
   },
   "source": [
    "# Make Your QA Pipelines Talk!\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://upload.wikimedia.org/wikipedia/en/d/d8/Game_of_Thrones_title_card.jpg\">\n",
    "\n",
    "Question answering works primarily on text, but Haystack provides some features for audio files that contain speech as well.\n",
    "\n",
    "In this tutorial, we're going to see how to use `AnswerToSpeech` to convert answers into audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7sF5gZnPgnG"
   },
   "source": [
    ">*Update:* AnswerToSpeech lives in the [text2speech](https://github.com/deepset-ai/haystack-extras/tree/main/nodes/text2speech) package. Main [Haystack](https://github.com/deepset-ai/haystack) reposity doesn't include it anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4UBjfz4LB3SS"
   },
   "source": [
    "### Prepare environment\n",
    "\n",
    "#### Colab: Enable the GPU runtime\n",
    "Make sure you enable the GPU runtime to experience decent speed in this tutorial.\n",
    "**Runtime -> Change Runtime type -> Hardware accelerator -> GPU**\n",
    "\n",
    "<img src=\"https://github.com/deepset-ai/haystack-tutorials/raw/main/tutorials/img/colab_gpu_runtime.jpg\">\n",
    "\n",
    "You can double check whether the GPU runtime is enabled with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDHmaD2gB3SX",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBvGUPVKN2oJ"
   },
   "source": [
    "To start, let's install the latest release of Haystack with `pip`. In this tutorial, we'll use components from [text2speech](https://github.com/deepset-ai/haystack-extras/tree/main/nodes/text2speech) which contains some extra Haystack components, so we'll install `farm-haystack-text2speech`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsY0HC8JB3Sc",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab]\n",
    "pip install farm-haystack-text2speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "r3kPhIxKN2oK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logging\n",
    "\n",
    "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
    "Example log message:\n",
    "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
    "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DyCNTb-zN2oK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbGu92rAB3Sl"
   },
   "source": [
    "## Indexing Documents\n",
    "\n",
    "First of all, we will populate the document store with a simple indexing pipeline. See [Tutorial 1](https://colab.research.google.com/github/deepset-ai/haystack/blob/main/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb) for more details about these steps.\n",
    "\n",
    "To the basic version, we can add here a DocumentToSpeech node that also generates an audio file for each of the indexed documents. This will make possible, during querying, to access the audio version of the documents the answers were extracted from without having to generate it on the fly.\n",
    "\n",
    "**Note**: this additional step can slow down your indexing quite a lot if you are not running on GPU. Experiment with very small corpora to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWYnP3nWB3So",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import FileTypeClassifier, TextConverter, PreProcessor\n",
    "\n",
    "# Initialize the DocumentStore\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "\n",
    "# Get the documents\n",
    "documents_path = \"data/tutorial17\"\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt17.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=documents_path)\n",
    "\n",
    "# List all the paths\n",
    "file_paths = [p for p in Path(documents_path).glob(\"**/*\")]\n",
    "\n",
    "# NOTE: In this example we're going to use only one text file from the wiki\n",
    "file_paths = [p for p in file_paths if \"Stormborn\" in p.name]\n",
    "\n",
    "# Prepare some basic metadata for the files\n",
    "files_metadata = [{\"name\": path.name} for path in file_paths]\n",
    "\n",
    "# Makes sure the file is a TXT file (FileTypeClassifier node)\n",
    "classifier = FileTypeClassifier()\n",
    "\n",
    "# Converts a file into text and performs basic cleaning (TextConverter node)\n",
    "text_converter = TextConverter(remove_numeric_tables=True)\n",
    "\n",
    "# - Pre-processes the text by performing splits and adding metadata to the text (Preprocessor node)\n",
    "preprocessor = PreProcessor(clean_header_footer=True, split_length=200, split_overlap=20)\n",
    "\n",
    "# Here we create a basic indexing pipeline\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_node(classifier, name=\"classifier\", inputs=[\"File\"])\n",
    "indexing_pipeline.add_node(text_converter, name=\"text_converter\", inputs=[\"classifier.output_1\"])\n",
    "indexing_pipeline.add_node(preprocessor, name=\"preprocessor\", inputs=[\"text_converter\"])\n",
    "indexing_pipeline.add_node(document_store, name=\"document_store\", inputs=[\"preprocessor\"])\n",
    "\n",
    "# Then we run it with the documents and their metadata as input\n",
    "output = indexing_pipeline.run(file_paths=file_paths, meta=files_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zW5qaqn1B3St"
   },
   "source": [
    "### Querying\n",
    "   \n",
    "Now we will create a pipeline very similar to the basic `ExtractiveQAPipeline` of Tutorial 1,\n",
    "with the addition of a node that converts our answers into audio files! Once the answer is retrieved, we can also listen to the audio version of the document where the answer came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_oecui1B3Sw"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import BM25Retriever, FARMReader\n",
    "from text2speech import AnswerToSpeech\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "answer2speech = AnswerToSpeech(\n",
    "    model_name_or_path=\"espnet/kan-bayashi_ljspeech_vits\", generated_audio_dir=Path(\"./audio_answers\")\n",
    ")\n",
    "\n",
    "audio_pipeline = Pipeline()\n",
    "audio_pipeline.add_node(retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "audio_pipeline.add_node(reader, name=\"Reader\", inputs=[\"Retriever\"])\n",
    "audio_pipeline.add_node(answer2speech, name=\"AnswerToSpeech\", inputs=[\"Reader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV1KHzXGB3Sy"
   },
   "source": [
    "## Ask a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-ZMUBzpB3Sz",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# You can configure how many candidates the Reader and Retriever shall return\n",
    "# The higher top_k_retriever, the better (but also the slower) your answers.\n",
    "prediction = audio_pipeline.run(\n",
    "    query=\"Who is the father of Arya Stark?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpFSxtNNB3S1"
   },
   "outputs": [],
   "source": [
    "# Now you can print prediction\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xg6BN4v8N2oM"
   },
   "outputs": [],
   "source": [
    "# The document the first answer was extracted from\n",
    "original_document = [doc for doc in prediction[\"documents\"] if doc.id == prediction[\"answers\"][0].document_ids[0]][0]\n",
    "pprint(original_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXf-kTn4B3S6"
   },
   "source": [
    "## Hear them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cJJVpT7dB3S7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRIotUuz-mpF"
   },
   "outputs": [],
   "source": [
    "prediction[\"answers\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usGVf1N6B3S8"
   },
   "outputs": [],
   "source": [
    "# The first answer in isolation\n",
    "\n",
    "print(\"Answer: \", prediction[\"answers\"][0].meta[\"answer_text\"])\n",
    "\n",
    "speech, _ = sf.read(prediction[\"answers\"][0].answer)\n",
    "display(Audio(speech, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTFwNJqtB3S9"
   },
   "outputs": [],
   "source": [
    "# The context of the first answer\n",
    "\n",
    "print(\"Context: \", prediction[\"answers\"][0].meta[\"context_text\"])\n",
    "\n",
    "speech, _ = sf.read(prediction[\"answers\"][0].context)\n",
    "display(Audio(speech, rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wJpoQQNdB3S-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
